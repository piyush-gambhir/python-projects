{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests bs4 lxml selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROXIES_JSON_FILE_PATH = 'proxies.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_proxies():\n",
    "#     try:\n",
    "#         with open(PROXIES_JSON_FILE_PATH, 'r') as file:\n",
    "#             proxies_data = json.load(file)\n",
    "#             proxies = [{'ip': proxy['ip'], 'port': proxy['port']}\n",
    "#                        for proxy in proxies_data]\n",
    "#             return proxies\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"The file {PROXIES_JSON_FILE_PATH} was not found.\")\n",
    "#         return []\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Error decoding the JSON file.\")\n",
    "#         return []\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    driver = webdriver.Firefox()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sitemap(url):\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    page_source = driver.page_source\n",
    "    driver.close()\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def parse_sitemap(sitemap_content):\n",
    "    soup = BeautifulSoup(sitemap_content, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    urls = [link.get('href') for link in links]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse(url):\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    time.sleep(30)\n",
    "    content = driver.page_source\n",
    "    content_length = int(len(content) * 0.65)\n",
    "    driver.close()\n",
    "    return BeautifulSoup(content[:content_length], 'html.parser')\n",
    "\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        if result.path.lower().endswith(('.png', '.svg', '.gif')):\n",
    "            return False\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_vin_and_images(soup, base_url):\n",
    "    vin = None\n",
    "    images = set()\n",
    "\n",
    "    vin_pattern = re.compile(r'[A-HJ-NPR-Z0-9]{17}')\n",
    "    for script in soup.find_all('script'):\n",
    "        if script.string:\n",
    "            match = vin_pattern.search(script.string)\n",
    "            if match:\n",
    "                vin = match.group()\n",
    "                break\n",
    "    print(f\"VIN: {vin}\")\n",
    "    gallery_div = soup.find('div', id='gallery-modal')\n",
    "    if gallery_div:\n",
    "        for img in gallery_div.find_all('img'):\n",
    "            # Check both 'src' and 'data-src' attributes for image URLs\n",
    "            src_urls = [img.get(attr)\n",
    "                        for attr in ['src', 'data-src'] if img.get(attr)]\n",
    "            for src_url in src_urls:\n",
    "                if src_url.startswith('/'):\n",
    "                    src_url = base_url + src_url\n",
    "                if is_valid_url(src_url):\n",
    "                    images.add(src_url)\n",
    "    print(f\"Images: {images}\")\n",
    "    return vin, list(images)\n",
    "\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def process_url(url, parsed_url, website_name, data):\n",
    "    soup = fetch_and_parse(url)\n",
    "    vin, images = extract_vin_and_images(\n",
    "        soup, base_url=f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "    )\n",
    "\n",
    "    if vin:\n",
    "        data['vins'][vin] = {'scraped_images_url': images}\n",
    "\n",
    "    with open(os.path.join(website_name, 'inventory_car_data.json'), 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def filter_urls(urls, pattern):\n",
    "    return [url for url in urls if pattern in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: Reached error page: about:neterror?e=dnsNotFound&u=https%3A//www.airportcdj.com/dealer-inspire-inventory/inventory_sitemap&c=UTF-8&d=We%20can%E2%80%99t%20connect%20to%20the%20server%20at%20www.airportcdj.com.\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\n",
      "UnknownError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:832:5\n",
      "checkReadyState@chrome://remote/content/marionette/navigate.sys.mjs:58:24\n",
      "onNavigation@chrome://remote/content/marionette/navigate.sys.mjs:330:39\n",
      "emit@resource://gre/modules/EventEmitter.sys.mjs:148:20\n",
      "receiveMessage@chrome://remote/content/marionette/actors/MarionetteEventsParent.sys.mjs:33:25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(url):\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    website_name = parsed_url.netloc\n",
    "    # check for other sitemap urls\n",
    "    sitemap_url = f'https://{website_name}/dealer-inspire-inventory/inventory_sitemap'\n",
    "\n",
    "    create_directory(website_name)\n",
    "    try:\n",
    "        sitemap_content = fetch_sitemap(sitemap_url)\n",
    "        urls = parse_sitemap(sitemap_content)\n",
    "        inventory_urls = filter_urls(urls, '/inventory')\n",
    "        with open(os.path.join(website_name, 'inventory_car_urls.txt'), 'w') as f:\n",
    "            f.writelines(\", \\n\".join(inventory_urls))\n",
    "\n",
    "        data = {\n",
    "            'website_url': website_name,\n",
    "            'vins': {}\n",
    "        }\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            futures = []\n",
    "            for inventory_url in inventory_urls:\n",
    "                futures.append(executor.submit(\n",
    "                    process_url, inventory_url, parsed_url, website_name, data))\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    website_input_url = 'https://www.airportcdj.com/inventory/new-2023-dodge-charger-sxt-rwd-sedan-2c3cdxbg9ph646474/'\n",
    "    main(website_input_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
