{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: scrapy in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.11.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (24.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (42.0.7)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (1.9.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (6.3)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (24.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.4)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tldextract->scrapy) (3.13.4)\n",
      "Requirement already satisfied: six in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests bs4 lxml scrapy selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "from lxml import html\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    driver = webdriver.Firefox()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sitemap(url):\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    time.sleep(20)\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def parse_sitemap(sitemap_content):\n",
    "    root = ET.fromstring(sitemap_content)\n",
    "    return [elem.text for elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse(url, driver):\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    content_length = int(len(content) * 0.65)\n",
    "    return BeautifulSoup(content[:content_length], 'html.parser')\n",
    "\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        if result.path.lower().endswith(('.png', '.svg', '.gif')):\n",
    "            return False\n",
    "\n",
    "        if 'logo' in result.path.lower():\n",
    "            return False\n",
    "\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_highest_resolution_image(srcset):\n",
    "    \"\"\"Extract the highest resolution image URL from the srcset.\"\"\"\n",
    "    highest_res = 0\n",
    "    highest_res_url = None\n",
    "    for entry in srcset.split(\",\"):\n",
    "        parts = entry.strip().split(\" \")\n",
    "        if len(parts) == 2 and parts[1].endswith(\"w\"):\n",
    "            res = int(parts[1][:-1])  # Remove 'w' and convert to integer\n",
    "            if res > highest_res:\n",
    "                highest_res = res\n",
    "                highest_res_url = parts[0]\n",
    "    return highest_res_url\n",
    "\n",
    "\n",
    "def extract_vin_and_images(soup, base_url, driver, VIN_X_PATH=None, IMAGE_CONTAINER_X_PATH=None):\n",
    "    vin = None\n",
    "    images = set()\n",
    "    vin_pattern = re.compile(r'[A-HJ-NPR-Z0-9]{17}')\n",
    "    vin = None\n",
    "    for text in soup.stripped_strings:\n",
    "        match = vin_pattern.search(text)\n",
    "        if match:\n",
    "            vin = match.group()\n",
    "            break\n",
    "    print(f\"VIN: {vin}\")\n",
    "\n",
    "    for img in soup.find_all('img'):\n",
    "        src_url = img.get('src') or img.get('data-src')\n",
    "        if src_url:\n",
    "            if src_url.startswith('/'):\n",
    "                src_url = base_url + src_url\n",
    "            if is_valid_url(src_url):\n",
    "                images.add(src_url)\n",
    "\n",
    "        srcset = img.get('srcset')\n",
    "        if srcset:\n",
    "            highest_res_image = get_highest_resolution_image(srcset)\n",
    "            if highest_res_image:\n",
    "                if highest_res_image.startswith('/'):\n",
    "                    highest_res_image = base_url + highest_res_image\n",
    "                if is_valid_url(highest_res_image):\n",
    "                    images.add(highest_res_image)\n",
    "\n",
    "        # Return the highest resolution images\n",
    "        highest_res_images = list(images)\n",
    "    print(f\"Images: {highest_res_images}\")\n",
    "    return vin, ', '.join(highest_res_images)\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def process_url(url, parsed_url, website_name, data, VIN_X_PATH=None, IMAGE_CONTAINER_X_PATH=None):\n",
    "    driver = setup_driver()\n",
    "    soup = fetch_and_parse(url, driver)\n",
    "    vin, images = extract_vin_and_images(\n",
    "        soup,\n",
    "        base_url=f\"{parsed_url.scheme}://{parsed_url.netloc}\",\n",
    "        driver=driver\n",
    "    )\n",
    "\n",
    "    if vin:\n",
    "        data['vins'][vin] = {'scraped_images_url': images}\n",
    "\n",
    "    with open(os.path.join(website_name, 'inventory_car_data.json'), 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, VIN_X_PATH=None, IMAGE_CONTAINER_X_PATH=None):\n",
    "    parsed_url = urlparse(url)\n",
    "    website_name = parsed_url.netloc\n",
    "    create_directory(website_name)\n",
    "    sitemap_url = f'https://{website_name}/sitemap.xml'\n",
    "    try:\n",
    "        sitemap_content = fetch_sitemap(sitemap_url)\n",
    "        all_urls = parse_sitemap(sitemap_content)\n",
    "\n",
    "        inventory_urls = []\n",
    "        for url in all_urls:\n",
    "            url_path_components = urlparse(url).path.split('/')\n",
    "            if (url_path_components[-1] != ''):\n",
    "                if (len(url_path_components) > 2):\n",
    "                    inventory_urls.append(url)\n",
    "                elif (len(url_path_components) >= 2 and url_path_components[1].count('-') >= parsed_url.path.split(\"/\")[1].count(\"-\") and any(char.isdigit() for char in url_path_components[1])):\n",
    "                    inventory_urls.append(url)\n",
    "\n",
    "        with open(os.path.join(website_name, 'inventory_car_urls.txt'), 'w') as f:\n",
    "            f.writelines(\", \\n\".join(inventory_urls))\n",
    "\n",
    "        data = {\n",
    "            website_name: website_name,\n",
    "            'vins': {}\n",
    "        }\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future_to_url = {executor.submit(process_url, url, parsed_url, website_name,\n",
    "                                             data, VIN_X_PATH, IMAGE_CONTAINER_X_PATH): url for url in inventory_urls}\n",
    "\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "\n",
    "                except Exception as exc:\n",
    "                    print(f\"{url} generated an exception: {exc}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # VIN_X_PATH = \"\"\"//*[@id=\"dealr-page\"]/div/div/div/div/div/div/div[4]/div/div/div/div/div[1]/div[2]\"\"\"\n",
    "    # IMAGE_CONTAINER_X_PATH = \"\"\"//*[@id=\"dealr-page\"]/div/div/div/div/div/div/div[3]/div/div[1]\"\"\"\n",
    "\n",
    "    website_input_url = 'https://www.extremecarcenter.com/details/used-2012-buick-verano/98039731'\n",
    "    main(website_input_url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
