{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mainp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests bs4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIN found: B1BGU1MMHF5WRB9VW\n",
      "VIN found: JTHKD5BH0F2216858\n",
      "VIN found: 58ADA1C17RU044659\n",
      "VIN found: 58ADA1C19RU045764\n",
      "An error occurred: [Errno 2] No such file or directory: 'www.sewelllexus-dallas.com\\\\data\\\\58ADA1C19RU045764\\\\10_0fd4ee08ca74b5ea25b2c79ccc564ef4x.jpg'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fetch_sitemap(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def parse_sitemap(sitemap_content):\n",
    "    root = ET.fromstring(sitemap_content)\n",
    "    return [elem.text for elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')]\n",
    "\n",
    "\n",
    "def filter_urls(urls, pattern):\n",
    "    return [url for url in urls if pattern in url]\n",
    "\n",
    "\n",
    "def fetch_and_parse(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_vin_and_images(soup):\n",
    "    \n",
    "    vin = soup.find('input', {'name': 'vin'})['value'] if soup.find(\n",
    "        'input', {'name': 'vin'}) else 'No VIN found'\n",
    "    images = [img['src'] for img in soup.select(\n",
    "        '#media1-app-root img[src]') if is_valid_url(img['src'])]\n",
    "    return vin, ', '.join(images)\n",
    "\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def download_image(image_url, save_path):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True,\n",
    "                                headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as out_file:\n",
    "                out_file.write(response.content)\n",
    "            # print(f\"Downloaded {image_url} to {save_path}\")\n",
    "        else:\n",
    "            # print(\n",
    "            #     f\"Failed to download {image_url} - Status code: {response.status_code}\")\n",
    "            pass\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while downloading {image_url}: {str(e)}\")\n",
    "\n",
    "\n",
    "def main(website_url):\n",
    "    parsed_url = urlparse(website_url)\n",
    "    website_name = parsed_url.netloc\n",
    "    base_directory = create_directory(website_name)\n",
    "    sitemap_url = f'https://{website_name}/sitemap.xml'\n",
    "    try:\n",
    "        sitemap_content = fetch_sitemap(sitemap_url)\n",
    "        all_urls = parse_sitemap(sitemap_content)\n",
    "        used_car_urls = filter_urls(all_urls, 'used/')\n",
    "        new_car_urls = filter_urls(all_urls, 'new/')\n",
    "\n",
    "        # Save filtered URLs\n",
    "        with open(os.path.join(website_name, 'used_car_urls.txt'), 'w') as f:\n",
    "            f.writelines(\"\\n\".join(used_car_urls))\n",
    "        with open(os.path.join(website_name, 'new_car_urls.txt'), 'w') as f:\n",
    "            f.writelines(\"\\n\".join(new_car_urls))\n",
    "\n",
    "        # Process each used car URL for images and VIN\n",
    "        data = []\n",
    "        for url in used_car_urls:\n",
    "            soup = fetch_and_parse(url)\n",
    "            vin, images = extract_vin_and_images(soup)\n",
    "            if images:  # Ensure there are valid images\n",
    "                # Adjust path for data subfolder\n",
    "                vin_directory = os.path.join(website_name, 'data', vin)\n",
    "                create_directory(vin_directory)\n",
    "                for index, image_url in enumerate(images.split(', ')):\n",
    "                    image_name = f\"{index}_{os.path.basename(urlparse(image_url).path)}\"\n",
    "                    save_path = os.path.join(vin_directory, image_name)\n",
    "                    download_image(image_url, save_path)\n",
    "                data.append((vin, images))\n",
    "\n",
    "            # Save data to CSV\n",
    "            if data:\n",
    "                with open(os.path.join(website_name, 'vin_image_data.csv'), 'w', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(['VIN', 'Image URLs'])\n",
    "                    for vin, images in data:\n",
    "                        writer.writerow([vin, images])\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_input_url = 'https://www.sewelllexus-dallas.com/'\n",
    "    main(website_input_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
